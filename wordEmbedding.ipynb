{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc4ux7DQq0ep0qAsd2iIRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishka55/hj/blob/main/wordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Li0RZH7MbS",
        "outputId": "6a2eb0ec-bae4-486f-d625-0aebfc1c0613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from pandas.io.parsers import read_csv\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import NMF\n",
        "from time import time\n",
        "from unicodedata import category\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "import pandas as pd\n",
        "#import pyLDAvis.sklearn \n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from itertools import combinations\n",
        "import gensim \n",
        "import csv\n",
        "from sklearn.preprocessing import normalize \n",
        "import statistics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scGugH_e7wZ9",
        "outputId": "1c4c47e7-d9eb-4b83-ab43-5ff060e68dde"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader import sentiwordnet\n",
        "class ContentModeling:\n",
        "        \n",
        "    def loadData(self, fields, inputfile):\n",
        "        \n",
        "        articles=[]\n",
        "        actual=[]\n",
        "           \n",
        "      \n",
        "        df=read_csv(inputfile, encoding = \"ISO-8859-1\").astype(str)\n",
        "     \n",
        "        for index, row in df.iterrows():\n",
        "            if len(row) > 1 :\n",
        "                actual.append(row[0])\n",
        "                articles.append(row[1])  \n",
        "            else:\n",
        "                articles.append(row[0])\n",
        "        del df\n",
        "        \n",
        "        return articles,actual\n",
        "\n",
        "    def prePros(self,text):\n",
        "        \"\"\" Tokenize text and lemmatize words removing punctuation \"\"\"\n",
        "        re_url = re.compile(r'(?:http|ftp|https)://(?:[\\\\w_-]+(?:(?:\\\\.[\\\\w_-]+)+))(?:[\\\\w.,@?^=%&:/~+#-]*[\\\\w@?^=%&/~+#-])?')\n",
        "        re_email = re.compile(r'[a-zA-Z][\\w\\.-]*[a-zA-Z0-9]@([a-zA-Z0-9][\\w\\.-]*[a-zA-Z0-9]\\.[a-zA-Z][a-zA-Z\\.]*[a-zA-Z])')\n",
        "        re2_email = re.compile(r'(?:[a-z0-9!#$%&*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')\n",
        "\n",
        "        # remove links\n",
        "        text = re.sub(r\"http\\S+\", \"\", str(text))\n",
        "        # remove special chars and numbers\n",
        "        #text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
        "        text = re.sub(r'(RT[^\\n]+\\n)', '', text)\n",
        "        text = re.sub(re_url, '', text)\n",
        "        text = re.sub(re_email, '', text)\n",
        "        text = re.sub(re2_email, '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        exclude = list(set(string.punctuation)) \n",
        "        text = ''.join(ch for ch in text if ch not in exclude)#delete punctuation\n",
        "        text = ''.join(ch for ch in text if category(ch)[0] != 'P') #delete punctuation ASCII as well as other\n",
        "        text = text.lower()\n",
        "\n",
        "        stop_words = stopwords.words(\"english\")\n",
        "        stop_words.extend([\"rt\",\"shouldnt\",\"ya\",\"yous\",\"youve\",\"youu\",\"pornandroidiphoneipadsexxxx\"]) # additional words to remove\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "        \n",
        "        return filtered_text\n",
        "\n",
        "\n",
        "    def articlestoSentence(self,art):\n",
        "      sentences=[]\n",
        "      for i in art:\n",
        "        sentences.append(d.prePros(i))\n",
        "      return sentences\n",
        "\n",
        "    def word2VecModel(self,sent):\n",
        "      model = Word2Vec(sent, min_count=1)\n",
        "      return model\n",
        "\n",
        "\n",
        "    def sent_vectorizer(self,sent,model):\n",
        "      sent_vec =[]\n",
        "      numw = 0\n",
        "      for w in sent:\n",
        "        try:\n",
        "          if numw == 0:\n",
        "            sent_vec = model[w]\n",
        "          else:\n",
        "            sent_vec = np.add(sent_vec, model[w])\n",
        "            numw+=1\n",
        "        except:\n",
        "            pass\n",
        "     \n",
        "      return np.asarray(sent_vec) / numw\n",
        "\n",
        "    def word2VecVector(self,sent,model):\n",
        "      model=model\n",
        "      X=[]\n",
        "      for sentence in sent:\n",
        "        X.append(d.sent_vectorizer(sentence,model))\n",
        "\n",
        "      return X\n"
      ],
      "metadata": {
        "id": "1zYeTYHu8AWz"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=ContentModeling()"
      ],
      "metadata": {
        "id": "UlGPgpLy8ydj"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_topics = 3 # number of topics you want to identify\n",
        "no_top_words = 15  # number of words to represent - making this too large will add low pobable words\n",
        "inputfile = '/content/drive/MyDrive/Colab Notebooks/labeled_data2.csv' #csv file containing short text : youtube.csv, twitter.csv or timesplitted datafiles\n",
        "fields=['class','tweet'] # column names of the csv file where text resides : 'text', 'description', 'title', 'earliest_block_message','claimant', 'earliest_block_long_message','earliest_block_short_message'\n"
      ],
      "metadata": {
        "id": "avjeLE-08WiF"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles,actual=d.loadData(fields,inputfile)\n",
        "sentences = d.articlestoSentence(articles)\n",
        "model = d.word2VecModel(sentences)\n",
        "X = d.word2VecVector(sentences,model)"
      ],
      "metadata": {
        "id": "qt2mV3AC8iyc"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRglj4yFEg5a",
        "outputId": "6611fe0e-c85a-424c-e153-f6212221bea4"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['mayasolovely',\n",
              "  'woman',\n",
              "  'complain',\n",
              "  'cleaning',\n",
              "  'house',\n",
              "  'amp',\n",
              "  'man',\n",
              "  'always',\n",
              "  'take',\n",
              "  'trash'],\n",
              " ['mleew',\n",
              "  'boy',\n",
              "  'dats',\n",
              "  'coldtyga',\n",
              "  'dwn',\n",
              "  'bad',\n",
              "  'cuffin',\n",
              "  'dat',\n",
              "  'hoe',\n",
              "  'st',\n",
              "  'place'],\n",
              " ['urkindofbrand',\n",
              "  'dawg',\n",
              "  'sbabylife',\n",
              "  'ever',\n",
              "  'fuck',\n",
              "  'bitch',\n",
              "  'start',\n",
              "  'cry',\n",
              "  'confused',\n",
              "  'shit'],\n",
              " ['cganderson', 'vivabased', 'look', 'like', 'tranny'],\n",
              " ['shenikaroberts',\n",
              "  'shit',\n",
              "  'hear',\n",
              "  'might',\n",
              "  'true',\n",
              "  'might',\n",
              "  'faker',\n",
              "  'bitch',\n",
              "  'told'],\n",
              " ['tmadisonx',\n",
              "  'shit',\n",
              "  'blows',\n",
              "  'meclaim',\n",
              "  'faithful',\n",
              "  'somebody',\n",
              "  'still',\n",
              "  'fucking',\n",
              "  'hoes'],\n",
              " ['brighterdays',\n",
              "  'sit',\n",
              "  'hate',\n",
              "  'another',\n",
              "  'bitch',\n",
              "  'got',\n",
              "  'much',\n",
              "  'shit',\n",
              "  'going'],\n",
              " ['selfiequeenbri',\n",
              "  'cause',\n",
              "  'im',\n",
              "  'tired',\n",
              "  'big',\n",
              "  'bitches',\n",
              "  'coming',\n",
              "  'us',\n",
              "  'skinny',\n",
              "  'girls'],\n",
              " ['amp', 'might', 'get', 'bitch', 'back', 'amp', 'thats'],\n",
              " ['rhythmixx', 'hobbies', 'include', 'fighting', 'mariam', 'bitch']]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAGlTVtf8_Tf",
        "outputId": "9be9678f-7607-4641-85b7-e83babfd70e7"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "        -inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32),\n",
              " array([-inf,  inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf,\n",
              "        -inf, -inf, -inf, -inf, -inf, -inf, -inf,  inf, -inf, -inf,  inf,\n",
              "         inf, -inf,  inf, -inf, -inf,  inf,  inf, -inf,  inf,  inf, -inf,\n",
              "        -inf,  inf,  inf, -inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf, -inf, -inf, -inf,  inf,  inf,  inf,  inf,  inf,\n",
              "         inf,  inf,  inf, -inf, -inf, -inf,  inf, -inf, -inf, -inf, -inf,\n",
              "        -inf, -inf,  inf,  inf,  inf,  inf,  inf,  inf, -inf, -inf,  inf,\n",
              "        -inf,  inf,  inf,  inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf,\n",
              "         inf,  inf, -inf,  inf, -inf,  inf,  inf, -inf, -inf, -inf, -inf,\n",
              "         inf], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    }
  ]
}